"use strict";(self.webpackChunktremor_website=self.webpackChunktremor_website||[]).push([[376],{83362:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>l,contentTitle:()=>o,default:()=>m,frontMatter:()=>s,metadata:()=>r,toc:()=>g});var i=n(58168),a=(n(96540),n(15680));n(40281);const s={},o="Influx",r={unversionedId:"recipes/influx/index",id:"version-0.11/recipes/influx/index",title:"Influx",description:"All the application code here is available from the docs git repository.",source:"@site/versioned_docs/version-0.11/recipes/11_influx/index.md",sourceDirName:"recipes/11_influx",slug:"/recipes/influx/",permalink:"/docs/0.11/recipes/influx/",draft:!1,editUrl:"https://github.com/tremor-rs/tremor-www/tree/main/versioned_docs/version-0.11/recipes/11_influx/index.md",tags:[],version:"0.11",frontMatter:{},sidebar:"version-0.11/tutorialSidebar",previous:{title:"Logstash",permalink:"/docs/0.11/recipes/logstash/"},next:{title:"PostgreSQL TimescaleDB",permalink:"/docs/0.11/recipes/postgres_timescaledb/"}},l={},g=[{value:"Environment",id:"environment",level:2},{value:"Business Logic",id:"business-logic",level:2},{value:"Grouping",id:"grouping",level:3},{value:"Aggregation",id:"aggregation",level:3},{value:"Normalisation to Influx Line Protocol",id:"normalisation-to-influx-line-protocol",level:3},{value:"Command line testing during logic development",id:"command-line-testing-during-logic-development",level:2},{value:"Discussion",id:"discussion",level:3}],p={toc:g},d="wrapper";function m(e){let{components:t,...s}=e;return(0,a.yg)(d,(0,i.A)({},p,s,{components:t,mdxType:"MDXLayout"}),(0,a.yg)("h1",{id:"influx"},"Influx"),(0,a.yg)("admonition",{type:"note"},(0,a.yg)("p",{parentName:"admonition"},"All the application code here is available from the docs ",(0,a.yg)("a",{parentName:"p",href:"https://github.com/tremor-rs/tremor-www/tree/main/versioned_docs/version-0.11/recipes/11_influx/index.md"},"git repository"),".")),(0,a.yg)("p",null,"This example demonstrates using Tremor as a proxy and aggregator for InfluxDB data. As such it coveres three topics. Ingesting and decoding influx data is the first part. Then grouping this data and aggregating over it."),(0,a.yg)("p",null,"The demo starts up a ",(0,a.yg)("a",{parentName:"p",href:"http://localhost:8888"},"local Chronograf"),". This allows browsing the data stored in influxdb. When first connecting you'll be asked to specify the database to use, please change the ","*","*","Connection URL","*","*"," to ",(0,a.yg)("inlineCode",{parentName:"p"},"http://influxdb:8086"),". For all other questions select ",(0,a.yg)("inlineCode",{parentName:"p"},"Skip")," as we do not need to configure those."),(0,a.yg)("p",null,"Once in Chronograf, look at the ",(0,a.yg)("inlineCode",{parentName:"p"},"tremor")," database to see the metrics and rollups. Since rollups do roll up over time you might have to wait a few minutes untill aggregated data propagates."),(0,a.yg)("p",null,"Depending on the performance of the system the demo is run on metrics may be shed due to tremors over load protection."),(0,a.yg)("h2",{id:"environment"},"Environment"),(0,a.yg)("p",null,"In the ",(0,a.yg)("a",{target:"_blank",href:n(92395).A},(0,a.yg)("code",null,"example.trickle"))," we process the data in multiple steps, since this is somewhat more complex then the prior examples we'll discuss each step in the Business Logic section."),(0,a.yg)("h2",{id:"business-logic"},"Business Logic"),(0,a.yg)("h3",{id:"grouping"},"Grouping"),(0,a.yg)("pre",null,(0,a.yg)("code",{parentName:"pre",className:"language-trickle"},'select {\n    "measurement": event.measurement,\n    "tags": event.tags,\n    "field": group[2],\n    "value": event.fields[group[2]],\n    "timestamp": event.timestamp,\n}\nfrom in\ngroup by set(event.measurement, event.tags, each(record::keys(event.fields)))\ninto aggregate\nhaving type::is_number(event.value);\n')),(0,a.yg)("p",null,"This step groups the data for aggregation. This is required since the ",(0,a.yg)("a",{parentName:"p",href:"https://docs.influxdata.com/influxdb/v1.7/write_protocols/line_protocol_tutorial/"},"Influx Line protocol")," allows for multiple values within one message. The grouping step ensures that we do not aggregate ",(0,a.yg)("inlineCode",{parentName:"p"},"cpu_idle")," and ",(0,a.yg)("inlineCode",{parentName:"p"},"cpu_user")," into the same value despite them being in the same result."),(0,a.yg)("p",null,"In other words we normalise an event like this"),(0,a.yg)("pre",null,(0,a.yg)("code",{parentName:"pre",className:"language-influx"},'measurement tag1=value1,tag2=value2 field1=42,field2="snot",field3=0.2 123587512345513\n')),(0,a.yg)("p",null,"into the three distinct series it represents, namely:"),(0,a.yg)("pre",null,(0,a.yg)("code",{parentName:"pre",className:"language-influx"},'measurement tag1=value1,tag2=value2 field1=42 123587512345513\nmeasurement tag1=value1,tag2=value2 field2="snot" 123587512345513\nmeasurement tag1=value1,tag2=value2 field3=0.2 123587512345513\n')),(0,a.yg)("p",null,"The second part that happens in this query is removing non numeric values from our aggregated series since they are not able to be aggregated."),(0,a.yg)("h3",{id:"aggregation"},"Aggregation"),(0,a.yg)("pre",null,(0,a.yg)("code",{parentName:"pre",className:"language-trickle"},'select\n{\n    "measurement": event.measurement,\n    "tags": patch event.tags of insert "window" => window end,\n    "stats": aggr::stats::hdr(event.value, [ "0.5", "0.9", "0.99", "0.999" ]),\n    "field": event.field,\n    "timestamp": aggr::win::first(event.timestamp), # we can\'t use min since it\'s a float\n}\nfrom aggregate[`10secs`, `1min`, ]\ngroup by set(event.measurement, event.tags, event.field)\ninto normalize;\n')),(0,a.yg)("p",null,"In this section we aggregate the different serieses we created in the previous section."),(0,a.yg)("p",null,"Most notably are the ",(0,a.yg)("inlineCode",{parentName:"p"},"aggr::stats::hdr")," and ",(0,a.yg)("inlineCode",{parentName:"p"},"aggr::win::first")," functions which do the aggregation. ",(0,a.yg)("inlineCode",{parentName:"p"},"aggr::stats::hdr")," uses a optimized ",(0,a.yg)("a",{parentName:"p",href:"http://hdrhistogram.org/"},"HDR Histogram")," algorithm to generate the values requested of it. ",(0,a.yg)("inlineCode",{parentName:"p"},"aggr::win::first")," gives the timestamp of the first event in the window."),(0,a.yg)("h3",{id:"normalisation-to-influx-line-protocol"},"Normalisation to Influx Line Protocol"),(0,a.yg)("pre",null,(0,a.yg)("code",{parentName:"pre",className:"language-tremor"},'select {\n  "measurement":  event.measurement,\n  "tags":  event.tags,\n  "fields":  {\n    "count_#{event.field}":  event.stats.count,\n    "min_#{event.field}":  event.stats.min,\n    "max_#{event.field}":  event.stats.max,\n    "mean_#{event.field}":  event.stats.mean,\n    "stdev_#{event.field}":  event.stats.stdev,\n    "var_#{event.field}":  event.stats.var,\n    "p50_#{event.field}":  event.stats.percentiles["0.5"],\n    "p90_#{event.field}":  event.stats.percentiles["0.9"],\n    "p99_#{event.field}":  event.stats.percentiles["0.99"],\n    "p99.9_#{event.field}":  event.stats.percentiles["0.999"]\n  },\n  "timestamp": event.timestamp,\n}\nfrom normalize\ninto batch;\n')),(0,a.yg)("p",null,"The last part normalises the data to a format that can be encoded into influx line protocol. And name the fields accordingly. This uses string interpolation for the recortd fields and simle value access for their values."),(0,a.yg)("h2",{id:"command-line-testing-during-logic-development"},"Command line testing during logic development"),(0,a.yg)("pre",null,(0,a.yg)("code",{parentName:"pre",className:"language-bash"},"$ docker-compose up\n  ... lots of logs ...\n")),(0,a.yg)("p",null,"Open the ",(0,a.yg)("a",{parentName:"p",href:"http://localhost:8888"},"Chronograf")," and connect the database."),(0,a.yg)("h3",{id:"discussion"},"Discussion"),(0,a.yg)("p",null,"It is noteworthy that in the aggregation context only ",(0,a.yg)("inlineCode",{parentName:"p"},"aggr::stats::hdr")," and ",(0,a.yg)("inlineCode",{parentName:"p"},"aggr::win::first")," are being evaluated for events, resulting record and the associated logic is only ever evaluated on emit."),(0,a.yg)("p",null,"We are using ",(0,a.yg)("inlineCode",{parentName:"p"},"having")," in the goruping step, however this could also be done with a ",(0,a.yg)("inlineCode",{parentName:"p"},"where")," clause on the aggregation step. In this example we choose ",(0,a.yg)("inlineCode",{parentName:"p"},"having")," over were as it is worth discarding events as early as possible. If the requirement were to handle non numeric fields in a different manner routing the output of the grouping step to two different select statements we would have used ",(0,a.yg)("inlineCode",{parentName:"p"},"where")," instead."),(0,a.yg)("admonition",{type:"tip"},(0,a.yg)("p",{parentName:"admonition"},"Using ",(0,a.yg)("inlineCode",{parentName:"p"},"aggr::win::first")," over ",(0,a.yg)("inlineCode",{parentName:"p"},"aggr::stats::min")," is a debatable choice as we use the timestamp of the first event not the minimal timestamp. Inside of tremor we do not re-order events so those two would result in the same result with ",(0,a.yg)("inlineCode",{parentName:"p"},"aggr::win::first")," being cheaper to run. In addition stats functions are currently implemented to return floating point numbers so ",(0,a.yg)("inlineCode",{parentName:"p"},"aggr::stats::min")," could lead incorrect timestamps we'd rather avoid.")))}m.isMDXComponent=!0},92395:(e,t,n)=>{n.d(t,{A:()=>i});const i=n.p+"assets/files/example-3ff32fa4c89f0fc03b95c7c12bb0a852.trickle"}}]);