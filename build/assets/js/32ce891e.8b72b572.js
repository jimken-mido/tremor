"use strict";(self.webpackChunktremor_website=self.webpackChunktremor_website||[]).push([[3551],{23128:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>c,default:()=>m,frontMatter:()=>i,metadata:()=>s,toc:()=>p});var r=t(58168),a=(t(96540),t(15680)),o=t(40281);const i={sidebar_label:"kafka",sidebar_position:1},c="The kafka Connector",s={unversionedId:"reference/connectors/kafka",id:"version-0.12/reference/connectors/kafka",title:"The kafka Connector",description:"The kafka connector provides integration with the Apache Kafka and compatible",source:"@site/versioned_docs/version-0.12/reference/connectors/kafka.md",sourceDirName:"reference/connectors",slug:"/reference/connectors/kafka",permalink:"/docs/0.12/reference/connectors/kafka",draft:!1,editUrl:"https://github.com/tremor-rs/tremor-www/tree/main/versioned_docs/version-0.12/reference/connectors/kafka.md",tags:[],version:"0.12",sidebarPosition:1,frontMatter:{sidebar_label:"kafka",sidebar_position:1},sidebar:"indexSidebar",previous:{title:"http",permalink:"/docs/0.12/reference/connectors/http"},next:{title:"kv (Key/Value Store)",permalink:"/docs/0.12/reference/connectors/kv"}},l={},p=[{value:"Configuration",id:"configuration",level:2},{value:"Consumer",id:"consumer",level:3},{value:"Producer",id:"producer",level:3},{value:"Kafka Echo Service example",id:"kafka-echo-service-example",level:2},{value:"Exercises",id:"exercises",level:2}],f={toc:p},d="wrapper";function m(e){let{components:n,...t}=e;return(0,a.yg)(d,(0,r.A)({},f,t,{components:n,mdxType:"MDXLayout"}),(0,a.yg)("h1",{id:"the-kafka-connector"},"The ",(0,a.yg)("inlineCode",{parentName:"h1"},"kafka")," Connector"),(0,a.yg)("p",null,"The ",(0,a.yg)("inlineCode",{parentName:"p"},"kafka")," connector provides integration with the ",(0,a.yg)("a",{parentName:"p",href:"https://kafka.apache.org/"},"Apache Kafka")," and compatible\nproducts such as ",(0,a.yg)("a",{parentName:"p",href:"https://www.confluent.io/"},"Confluent Kafka")," and ",(0,a.yg)("a",{parentName:"p",href:"https://redpanda.com/"},"Redpanda"),"."),(0,a.yg)("h2",{id:"configuration"},"Configuration"),(0,a.yg)("p",null,"This section illustrates the configuration options for consumers and producers."),(0,a.yg)("p",null,"The connector is built on top of ",(0,a.yg)("a",{parentName:"p",href:"https://github.com/edenhill/librdkafka"},"librdkafka")," and exposes the full complement\nof ",(0,a.yg)("a",{parentName:"p",href:"https://github.com/edenhill/librdkafka/blob/master/CONFIGURATION.md"},"configuration settings"),". Care ",(0,a.yg)("strong",{parentName:"p"},"SHOULD")," be\ntaken when configuring ",(0,a.yg)("inlineCode",{parentName:"p"},"kafka")," with tremor to ensure that the configuration settings make sense given the logic\nrequired of the resulting system."),(0,a.yg)("h3",{id:"consumer"},"Consumer"),(0,a.yg)("p",null,"Example configuration as a consumer."),(0,a.yg)("pre",null,(0,a.yg)("code",{parentName:"pre",className:"language-tremor",metastring:'title="config.troy"',title:'"config.troy"'},'define connector consumer from kafka_consumer\nwith\n    # Enables metrics at a 1 second interval\n    metrics_interval_s = 1,\n\n    # Reconnection logic\n    reconnect = {\n            "retry": {\n                "interval_ms": 3000,\n                "max_retries": 10\n            }\n        },\n\n    # Data body content is JSON formatted\n    codec = "json",\n\n    # Kafka specific consumer configuration\n    config = {\n    # required - List of broker bootstrap servers\n            "brokers": [\n                "127.0.0.1:9092"\n            ],\n\n    # required - Consumer group id\n            "group_id": "test1",\n\n    # required - list of subscription topics to register with\n            "topics": [\n                "tremor_test"\n            ],\n\n    # optional - Whether or not to retry failed attempts\n            # When true - resets the offset to a failed message for retry\n            #  - Warning: where persistent failure is expected, this will lead to persistent errors\n            # When false - Only commits offset for a successful acknowledgement\n            "retry_failed_events": false,\n\n    # optional - librdkafka configuration settings ( indicative illustrative example )\n            "rdkafka_options": {\n                "enable.auto.commit": "false",\n                "auto.offset.reset": "beginning",\n            }\n        }\nend;\n')),(0,a.yg)("h3",{id:"producer"},"Producer"),(0,a.yg)("p",null,"Example configuration as a producer."),(0,a.yg)("pre",null,(0,a.yg)("code",{parentName:"pre",className:"language-tremor"},'    define connector producer from kafka_producer\n    with\n        # Enables metrics at a 1 second interval\n        metrics_interval_s = 1,\n\n        # Reconnection logic\n        reconnect = {\n            "retry": {\n                "interval_ms": 3000,\n                "max_retries": 10\n            }\n        },\n\n        # Data body content is JSON formatted\n        codec = "json",\n\n    # Kafka specific producer configuration\n        config = {\n        # required - List of broker bootstrap servers\n            "brokers": [\n                "127.0.0.1:9092",\n            ],\n\n            # required - the topic to send to\n            "topic": "tremor_test",\n\n            # optional - Key for messages, overwritten by `kafka.key` in metadata if present\n            "key": "snot",\n\n            # optional - librdkafka configuration settings ( indicative illustrative example )\n            # "rdkafka_options": { ... },\n        }\n    end;\n')),(0,a.yg)("h2",{id:"kafka-echo-service-example"},"Kafka Echo Service example"),(0,a.yg)("p",null,"A complete kafka example echo service that bubbles consumed events to a topic."),(0,a.yg)("p",null,"In this example both the producer and consumer are connected to the same kafka\ncluster and consume from and produce to the same topic."),(0,a.yg)(o.K,{chart:"graph LR\n    A{Kafka Consumer} --\x3e|topic_test events| B(batch)\n    B --\x3e|echo| C{Kafka Producer}\n    A --\x3e|connect to| K[Kafka Cluster]\n    C --\x3e |connect to| K[Kafka Cluster]",mdxType:"Mermaid"}),(0,a.yg)("p",null,"The actual logic is a little more verbose. However, the basic structure will be similar for\nother Kafka ",(0,a.yg)("inlineCode",{parentName:"p"},"consumer")," and ",(0,a.yg)("inlineCode",{parentName:"p"},"producer")," configurations and can be modularised."),(0,a.yg)("pre",null,(0,a.yg)("code",{parentName:"pre",className:"language-tremor",metastring:'title="config.troy"',title:'"config.troy"'},'# Kafka producer flow\ndefine flow kafka_produce\nflow\n\n    use tremor::connectors;\n    use tremor::pipelines;\n    use integration;\n\n    # Producer Kafka connector\n    define connector producer from kafka_producer\n    with\n        metrics_interval_s = 1,\n        reconnect = {\n            "retry": {\n                "interval_ms": 3000,\n                "max_retries": 10\n            }\n        },\n        codec = "json",\n        config = {\n            "brokers": [\n                "127.0.0.1:9092",\n            ],\n            "topic": "tremor_test",\n            "key": "snot"\n        }\n    end;\n\n    # Producer logic\n    define pipeline produce\n    pipeline\n        use std::time::nanos;\n\n        define script add_kafka_meta\n        script\n            let $kafka_producer = event.meta;\n            emit event["event"]\n        end;\n        create script add_kafka_meta;\n\n        # Batch events by two or emit after 1 second otherwise\n        define operator batch from generic::batch\n        with\n            count = 2,\n            timeout = nanos::from_seconds(1)\n        end;\n        create operator batch;\n\n        select event from in into add_kafka_meta;\n\n        select event from add_kafka_meta \n        where \n            match event of\n              case %{ batch == true } => false\n              default => true\n            end\n        into out;\n        select event from add_kafka_meta \n        where \n            match event of \n              case %{ batch == true } => true \n              default => false \n            end \n        into batch;\n        select event from add_kafka_meta/err into err;\n\n        select event from batch/err into err;\n        select event from batch into out;\n    end;\n    \n    create connector read_file from integration::read_file;\n    create connector producer;\n    create connector stderr from connectors::console;\n\n    create pipeline passthrough from pipelines::passthrough;\n    create pipeline produce from produce;\n\n    connect /connector/read_file to /pipeline/produce;\n    connect /connector/read_file/err to /pipeline/passthrough;\n    connect /pipeline/produce/out to /connector/producer;\n    connect /pipeline/produce/err to /connector/stderr/stderr;\n    connect /pipeline/passthrough to /connector/stderr/stderr;\n\nend;\n\n# Kafka consumer flow\ndefine flow kafka_consume\nflow\n    use tremor::connectors;\n    use tremor::pipelines;\n    use integration;\n\n    # Consumer Kafka connector\n    define connector consumer from kafka_consumer\n    with\n        metrics_interval_s = 1,\n        reconnect = {\n                "retry": {\n                    "interval_ms": 3000,\n                    "max_retries": 10\n                }\n            },\n        codec = "json",\n        config = {\n                "brokers": [\n                    "127.0.0.1:9092"\n                ],\n                "group_id": "test1",\n                "topics": [\n                    "tremor_test"\n                ],\n                "retry_failed_events": false,\n                "rdkafka_options": {\n                    "enable.auto.commit": "false",\n                    "auto.offset.reset": "beginning",\n                }\n            }\n    end;\n\n    define pipeline consume\n    into out, exit, err\n    pipeline\n        define script clean_kafka_meta\n        script\n            use std::string;\n            let $kafka_consumer.key = string::from_utf8_lossy($kafka_consumer.key);\n            let $kafka_consumer.timestamp = null;\n            event\n        end;\n        create script clean_kafka_meta;\n\n        select event from in into clean_kafka_meta;\n        select {"event": event, "meta": $} from clean_kafka_meta where event != "exit" into out;\n        select event from clean_kafka_meta where event == "exit" into exit;\n        select event from clean_kafka_meta/err into err;\n    end;\n    \n    create connector exit from integration::exit;\n    create connector write_file from integration::write_file;\n    create connector consumer;\n    create connector stderr from connectors::console;\n\n    create pipeline consume;\n    create pipeline passthrough from pipelines::passthrough;\n\n    # main logic\n    connect /connector/consumer to /pipeline/consume;\n    connect /pipeline/consume/out to /connector/write_file;\n    connect /pipeline/consume/exit to /connector/exit;\n\n    # debugging\n    connect /connector/consumer/err to /pipeline/passthrough;\n    connect /pipeline/consume/err to /connector/stderr/stderr;\n    connect /pipeline/passthrough to /connector/stderr/stderr;\nend;\n\ndeploy flow kafka_produce;\ndeploy flow kafka_consume;\n')),(0,a.yg)("h2",{id:"exercises"},"Exercises"),(0,a.yg)("ul",null,(0,a.yg)("li",{parentName:"ul"},"Modify the example to introduce guaranteed delivery of the tremor logic based on the ",(0,a.yg)("a",{parentName:"li",href:"./wal"},"wal")," write-ahead log connector"),(0,a.yg)("li",{parentName:"ul"},"Use a separate cluster for the consumer and producer"),(0,a.yg)("li",{parentName:"ul"},"Modify the ",(0,a.yg)("inlineCode",{parentName:"li"},"rdkafka_options")," configuration to reflect production configuration in your system"),(0,a.yg)("li",{parentName:"ul"},"Use ",(0,a.yg)("inlineCode",{parentName:"li"},"$kafka_consumer.key")," metadata from received kafka events"),(0,a.yg)("li",{parentName:"ul"},"Use ",(0,a.yg)("inlineCode",{parentName:"li"},"$kafka_consumer.headers")," metadata from received kafka events"),(0,a.yg)("li",{parentName:"ul"},"Use ",(0,a.yg)("inlineCode",{parentName:"li"},"$kafka_consumer.topic")," metadata from received kafka events"),(0,a.yg)("li",{parentName:"ul"},"Use ",(0,a.yg)("inlineCode",{parentName:"li"},"$kafka_consumer.partition")," metadata from received kafka events"),(0,a.yg)("li",{parentName:"ul"},"Use ",(0,a.yg)("inlineCode",{parentName:"li"},"$kafka_consumer.offset")," metadata from received kafka events"),(0,a.yg)("li",{parentName:"ul"},"Use ",(0,a.yg)("inlineCode",{parentName:"li"},"$kafka_consumer.timestamp")," metadata from received kafka events"),(0,a.yg)("li",{parentName:"ul"},"Set ",(0,a.yg)("inlineCode",{parentName:"li"},"$kafka_producer.headers")," metadata to propagate kafka header metadata"),(0,a.yg)("li",{parentName:"ul"},"Set ",(0,a.yg)("inlineCode",{parentName:"li"},"$kafka_producer.timestamp")," metadata to alter timestamp metadata"),(0,a.yg)("li",{parentName:"ul"},"Set ",(0,a.yg)("inlineCode",{parentName:"li"},"$kafka_producer.partition")," metadata to alter kafka partition metadata")))}m.isMDXComponent=!0}}]);